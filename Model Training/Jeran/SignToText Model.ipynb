{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc14b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b887d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATHS ---\n",
    "MODEL_PATH = r\"C:\\Users\\HP\\Desktop\\RP\\RP\\Model Training\\Jeran\\action.h5\"\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\Nirmala.ttf\" # Fix: Cleaned up the space before '='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e770d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "print(\"Loading Model...\")\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c1fc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Font\n",
    "try:\n",
    "    # Font size increased from 40 to 50 (Removed problematic Unicode character from comment)\n",
    "    font = ImageFont.truetype(FONT_PATH, 50)\n",
    "except:\n",
    "    print(\"Warning: Nirmala Font not found. Using default.\")\n",
    "    font = ImageFont.load_default()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "515d0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\n",
    "    'Beautiful', 'Black', 'Drink', 'Eat', 'Five',\n",
    "    'Good_morning', 'Good_night', 'Hello',\n",
    "    'Love', 'Mother', 'One', 'Run', 'Sleep', 'Talk', 'Ten',\n",
    "    'Three', 'Today', 'Tomorrow', 'Two', 'Yesterday'\n",
    "])\n",
    "\n",
    "translate = {\n",
    "    'Beautiful': {'en': 'Beautiful', 'ta': 'அழகு'},\n",
    "    'Black': {'en': 'Black', 'ta': 'கருப்பு'},\n",
    "    'Drink': {'en': 'Drink', 'ta': 'குடி'},\n",
    "    'Eat': {'en': 'Eat', 'ta': 'சாப்பிடு'},\n",
    "    'Five': {'en': 'Five', 'ta': 'ஐந்து'},\n",
    "    'Good_morning': {'en': 'Good morning', 'ta': 'காலை வணக்கம்'},\n",
    "    'Good_night': {'en': 'Good night', 'ta': 'இனிய இரவு'},\n",
    "    'Hello': {'en': 'Hello', 'ta': 'வணக்கம்'},\n",
    "    'Love': {'en': 'Love', 'ta': 'காதல்'},\n",
    "    'Mother': {'en': 'Mother', 'ta': 'அம்மா'},\n",
    "    'One': {'en': 'One', 'ta': 'ஒன்று'},\n",
    "    'Run': {'en': 'Run', 'ta': 'ஓடு'},\n",
    "    'Sleep': {'en': 'Sleep', 'ta': 'தூங்கு'},\n",
    "    'Talk': {'en': 'Talk', 'ta': 'பேசு'},\n",
    "    'Ten': {'en': 'Ten', 'ta': 'பத்து'},\n",
    "    'Three': {'en': 'Three', 'ta': 'மூன்று'},\n",
    "    'Today': {'en': 'Today', 'ta': 'இன்று'},\n",
    "    'Tomorrow': {'en': 'Tomorrow', 'ta': 'நாளை'},\n",
    "    'Two': {'en': 'Two', 'ta': 'இரண்டு'},\n",
    "    'Yesterday': {'en': 'Yesterday', 'ta': 'நேற்று'}\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11b92ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6511b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function 1: Unicode Text Drawing\n",
    "def draw_unicode_text(img, text, position, color):\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Helper Function 2: Keypoint Extraction (258 features)\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d1c986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI STARTING... DO AN ACTION!\n"
     ]
    }
   ],
   "source": [
    "s# MAIN LOOP STARTING HERE\n",
    "sequence = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set Camera Resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create Resizable Window\n",
    "cv2.namedWindow('SIGN SIGHT', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('SIGN SIGHT', 1000, 600)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    print(\"AI STARTING... DO AN ACTION!\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # 1. Convert to RGB for MediaPipe processing\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 2. Process results (Run the MediaPipe Holistic model)\n",
    "        results = holistic.process(image_rgb)\n",
    "        \n",
    "        # 3. Use the original frame as our canvas (It is already BGR)\n",
    "        image = frame \n",
    "        \n",
    "        # Draw keypoint connections on the frame\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # Logic for collecting keypoints\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        \n",
    "        # Keep only the last 30 frames (time window for the sign)\n",
    "        sequence = sequence[-30:] \n",
    "\n",
    "        # STATUS BAR\n",
    "        cv2.rectangle(image, (0,0), (1280, 60), (0,0,0), -1) # Status Bar Height increased to 60\n",
    "        image = draw_unicode_text(image, \"Status: Camera ON | Waiting for Action...\", (10, 5), (200, 200, 200)) # Status Text position adjusted\n",
    "\n",
    "        # Prediction\n",
    "        if len(sequence) == 30:\n",
    "            prediction = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "            action = actions[np.argmax(prediction)]\n",
    "            confidence = np.max(prediction)\n",
    "\n",
    "            # SHOW LIVE CONFIDENCE TEXT\n",
    "            conf_text = f\"Detected: {action} ({int(confidence*100)}%)\"\n",
    "            # Detected Text position adjusted\n",
    "            image = draw_unicode_text(image, conf_text, (10, 70), (0, 255, 0)) \n",
    "\n",
    "            if confidence > 0.5:\n",
    "                # English Translation Display\n",
    "                image = draw_unicode_text(image, f\"EN: {translate.get(action, {'en': 'Unknown'})['en']}\", (30, 150), (0, 255, 0))\n",
    "                # Tamil Translation Display\n",
    "                image = draw_unicode_text(image, f\"TA: {translate.get(action, {'ta': 'தெரியாதது'})['ta']}\", (30, 220), (0, 255, 255))\n",
    "                # Sinhala section removed\n",
    "\n",
    "        cv2.imshow('SIGN SIGHT', image)\n",
    "        \n",
    "        # Break gracefully on 'q' press\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5f9f5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b84c5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cd8cffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df9d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c8ff3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcf842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1ba916-1bc7-48f9-9e16-94b4777c02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "AI STARTING... DO AN ACTION!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f71327-48b7-47ac-8ab9-0bbd98e29299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08902171-bd0f-4a1a-9814-a749a03d6fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44aa4b-93ab-474c-9557-257991387e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc813f1c-3228-4a90-bc6a-5698c7da7ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mediapipe Env",
   "language": "python",
   "name": "mp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
